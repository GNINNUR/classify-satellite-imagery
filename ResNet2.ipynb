{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet Model Class implementation: \n",
    "# https://github.com/raghakot/keras-resnet/blob/master/resnet.py\n",
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=init_strides,\n",
    "                           padding=\"same\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
    "                              strides=init_strides,\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                     strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "        \"\"\"Builds a custom ResNet like architecture.\n",
    "\n",
    "        Args:\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
    "                The original paper used basic_block for layers < 50\n",
    "            repetitions: Number of repetitions of various block units.\n",
    "                At each block unit, the number of filters are doubled and the input size is halved\n",
    "\n",
    "        Returns:\n",
    "            The keras `Model`.\n",
    "        \"\"\"\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "                                 strides=(1, 1))(block)\n",
    "        flatten1 = Flatten()(pool2)\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
    "                      activation=\"sigmoid\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Keras libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [08:48<00:00, 76.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "# referred to https://www.kaggle.com/anokas/simple-keras-starter for help reading data and setting up basic Keras model\n",
    "x = []\n",
    "x_test = []\n",
    "y = []\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('/home/joerj/train-jpg/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x.append(cv2.resize(img, (32, 32)))\n",
    "    y.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('resnet18_amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 35000\n",
    "x_train, x_valid, y_train, y_valid = x[:split], x[split:], y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect();\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 17\n",
    "data_augmentation = True\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 3\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train -= mean_image\n",
    "x_valid -= mean_image\n",
    "x_train /= 128.\n",
    "x_valid /= 128.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement tracking of the best model \n",
    "filepath=\"weights_resnet.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43603258119310651, 0.23485001093660082, 0.19049741181646074]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.9308Epoch 00000: val_acc improved from -inf to 0.91903, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 49s - loss: 0.4328 - acc: 0.9308 - val_loss: 0.3328 - val_acc: 0.9190\n",
      "Epoch 2/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9425Epoch 00001: val_acc improved from 0.91903 to 0.94199, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.2315 - acc: 0.9425 - val_loss: 0.2188 - val_acc: 0.9420\n",
      "Epoch 3/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9444Epoch 00002: val_acc improved from 0.94199 to 0.94271, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.1895 - acc: 0.9444 - val_loss: 0.1857 - val_acc: 0.9427\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9450Epoch 00000: val_acc improved from 0.94271 to 0.94369, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 38s - loss: 0.1735 - acc: 0.9450 - val_loss: 0.1724 - val_acc: 0.9437\n",
      "Epoch 2/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9464Epoch 00001: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.1636 - acc: 0.9464 - val_loss: 0.1700 - val_acc: 0.9432\n",
      "Epoch 3/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9476Epoch 00002: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.1576 - acc: 0.9476 - val_loss: 0.1818 - val_acc: 0.9377\n",
      "Epoch 4/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9480Epoch 00003: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.1548 - acc: 0.9479 - val_loss: 0.1683 - val_acc: 0.9399\n",
      "Epoch 5/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9482Epoch 00004: val_acc improved from 0.94369 to 0.94583, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.1526 - acc: 0.9482 - val_loss: 0.1587 - val_acc: 0.9458\n",
      "Epoch 6/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9494Epoch 00005: val_acc improved from 0.94583 to 0.94605, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.1494 - acc: 0.9494 - val_loss: 0.1577 - val_acc: 0.9461\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9545Epoch 00000: val_acc improved from 0.94605 to 0.95277, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 38s - loss: 0.1339 - acc: 0.9545 - val_loss: 0.1370 - val_acc: 0.9528\n",
      "Epoch 2/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9560Epoch 00001: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.1283 - acc: 0.9560 - val_loss: 0.1403 - val_acc: 0.9499\n",
      "Epoch 3/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9573Epoch 00002: val_acc improved from 0.95277 to 0.95279, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.1249 - acc: 0.9573 - val_loss: 0.1346 - val_acc: 0.9528\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9582Epoch 00000: val_acc improved from 0.95279 to 0.95305, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 39s - loss: 0.1219 - acc: 0.9582 - val_loss: 0.1342 - val_acc: 0.9531\n",
      "Epoch 2/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9593Epoch 00001: val_acc improved from 0.95305 to 0.95340, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 34s - loss: 0.1190 - acc: 0.9593 - val_loss: 0.1336 - val_acc: 0.9534\n",
      "Epoch 3/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9604Epoch 00002: val_acc improved from 0.95340 to 0.95354, saving model to weights_resnet.best.hdf5\n",
      "35000/35000 [==============================] - 33s - loss: 0.1161 - acc: 0.9604 - val_loss: 0.1344 - val_acc: 0.9535\n",
      "Epoch 4/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9620Epoch 00003: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.1126 - acc: 0.9620 - val_loss: 0.1356 - val_acc: 0.9525\n",
      "Epoch 5/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9635Epoch 00004: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.1087 - acc: 0.9635 - val_loss: 0.1379 - val_acc: 0.9533\n",
      "Epoch 6/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9653Epoch 00005: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.1045 - acc: 0.9653 - val_loss: 0.1390 - val_acc: 0.9530\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9709Epoch 00000: val_acc did not improve\n",
      "35000/35000 [==============================] - 37s - loss: 0.0917 - acc: 0.9709 - val_loss: 0.1424 - val_acc: 0.9527\n",
      "Epoch 2/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9727Epoch 00001: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.0877 - acc: 0.9727 - val_loss: 0.1458 - val_acc: 0.9519\n",
      "Epoch 3/3\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9738Epoch 00002: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.0850 - acc: 0.9738 - val_loss: 0.1490 - val_acc: 0.9513\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9746Epoch 00000: val_acc did not improve\n",
      "35000/35000 [==============================] - 38s - loss: 0.0829 - acc: 0.9746 - val_loss: 0.1523 - val_acc: 0.9511\n",
      "Epoch 2/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9755Epoch 00001: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.0807 - acc: 0.9755 - val_loss: 0.1544 - val_acc: 0.9515\n",
      "Epoch 3/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9765Epoch 00002: val_acc did not improve\n",
      "35000/35000 [==============================] - 32s - loss: 0.0783 - acc: 0.9766 - val_loss: 0.1566 - val_acc: 0.9508\n",
      "Epoch 4/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9773Epoch 00003: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.0765 - acc: 0.9772 - val_loss: 0.1607 - val_acc: 0.9511\n",
      "Epoch 5/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9784Epoch 00004: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.0742 - acc: 0.9784 - val_loss: 0.1638 - val_acc: 0.9504\n",
      "Epoch 6/6\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9792Epoch 00005: val_acc did not improve\n",
      "35000/35000 [==============================] - 33s - loss: 0.0723 - acc: 0.9792 - val_loss: 0.1663 - val_acc: 0.9503\n"
     ]
    }
   ],
   "source": [
    "model = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "\n",
    "scores_list = []\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "epochs_arr = [3, 6]\n",
    "learn_rates = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "for learn_rate in learn_rates:\n",
    "    for epochs in epochs_arr:\n",
    "        history = LossHistory()\n",
    "        opt = Adam(lr=learn_rate)\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs =epochs,\n",
    "                      validation_data=(x_valid, y_valid),\n",
    "                      callbacks=[history, checkpoint, early_stopper, csv_logger])\n",
    "        \n",
    "        p_valid = model.predict(x_valid)\n",
    "        score = fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples')\n",
    "\n",
    "        train_loss_list.extend(history.train_losses)\n",
    "        valid_loss_list.extend( history.val_losses)\n",
    "        scores_list.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"loss_Resnet.csv\", np.vstack((valid_loss_list, train_loss_list)), fmt='%.18e', delimiter=',')\n",
    "np.savetxt(\"scores.csv\", scores_list, fmt='%.18e', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
