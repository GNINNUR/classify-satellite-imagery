{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Keras libraries \n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [10:48<00:00, 66.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# referred to https://www.kaggle.com/anokas/simple-keras-starter for help reading data and setting up basic Keras model\n",
    "x = []\n",
    "x_test = []\n",
    "y = []\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x.append(cv2.resize(img, (32, 32)))\n",
    "    y.append(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array(y, np.uint8)\n",
    "x = np.array(x, np.float16) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = 35000\n",
    "x_train, x_valid, y_train, y_valid = x[:split], x[split:], y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/4\n",
      "35000/35000 [==============================] - 294s - loss: 0.2700 - acc: 0.8990 - val_loss: 0.2127 - val_acc: 0.9147\n",
      "Epoch 2/4\n",
      "35000/35000 [==============================] - 279s - loss: 0.1950 - acc: 0.9262 - val_loss: 0.1707 - val_acc: 0.9340\n",
      "Epoch 3/4\n",
      "35000/35000 [==============================] - 275s - loss: 0.1757 - acc: 0.9325 - val_loss: 0.1517 - val_acc: 0.9406\n",
      "Epoch 4/4\n",
      "35000/35000 [==============================] - 284s - loss: 0.1666 - acc: 0.9358 - val_loss: 0.1491 - val_acc: 0.9414\n",
      "('Current results', 'F1: ', 0.86129869286543659, 'threshold: ', 0.17999999999999997, ' Dropout: ', 0.3045845721625034, ' Filter Size: ', 64)\n",
      "('New best F1 found with', 'F1: ', 0.86129869286543659, 'threshold: ', 0.17999999999999997, ' Dropout: ', 0.3045845721625034, ' Filter Size: ', 64)\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/4\n",
      "35000/35000 [==============================] - 302s - loss: 0.3673 - acc: 0.8440 - val_loss: 0.2281 - val_acc: 0.9058\n",
      "Epoch 2/4\n",
      " 9472/35000 [=======>......................] - ETA: 214s - loss: 0.2603 - acc: 0.9039"
     ]
    }
   ],
   "source": [
    "#Random Search over filter sizes and dropout\n",
    "\n",
    "best_f = -1\n",
    "best_d = -1\n",
    "best_thresh = 0\n",
    "best_F1 = -1 \n",
    "\n",
    "num_experiments = 3\n",
    "\n",
    "for i in xrange(num_experiments):\n",
    "    fSize = np.random.choice((32, 64))\n",
    "    dProb = np.random.uniform(low = 0.1, high = 0.8)\n",
    "    \n",
    "    #Model set up and fitting\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(fSize, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(fSize, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dProb))\n",
    "\n",
    "    model.add(Conv2D(fSize, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(fSize, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(dProb))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dProb))\n",
    "    model.add(Dense(17, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          validation_data=(x_valid, y_valid))\n",
    "    #grid search for best threshold on training data \n",
    "    p_train = model.predict(x_train, batch_size=128)\n",
    "\n",
    "    best_F1_train = -1\n",
    "    for t in np.arange(.1, .3, .01):\n",
    "        F1 = fbeta_score(y_train, np.array(p_train) > t, beta=2, average='samples')\n",
    "        if F1 > best_F1_train:\n",
    "            thresh = t\n",
    "            best_F1_train = F1\n",
    "\n",
    "    p_valid = model.predict(x_valid, batch_size=128)\n",
    "    \n",
    "    F1 = fbeta_score(y_valid, np.array(p_valid) > thresh, beta=2, average='samples')\n",
    "    print(\"Current results\", \"F1: \" , F1, \"threshold: \", thresh , \" Dropout: \", dProb, \" Filter Size: \", fSize)\n",
    "    if F1 > best_F1:\n",
    "        best_f = fSize\n",
    "        best_d = dProb\n",
    "        best_thresh = thresh\n",
    "        best_F1 = F1\n",
    "        print(\"New best F1 found with\", \"F1: \" , best_F1, \"threshold: \", best_thresh , \" Dropout: \", best_d, \" Filter Size: \", best_f)\n",
    "\n",
    "#('New best F1 found with', 'F1: ', 0.86129869286543659, 'threshold: ', 0.17999999999999997, ' Dropout: ', 0.3045845721625034, ' Filter Size: ', 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
