{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0-rc1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Uses tf.contrib.data module which is in release candidate 1.2.0rc0\n",
    "Based on:\n",
    "    - PyTorch example from Justin Johnson:\n",
    "      https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c\n",
    "      - https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c\n",
    "Required packages: tensorflow (v1.2)\n",
    "You can install the release candidate 1.2.0rc0 here:\n",
    "https://www.tensorflow.org/versions/r1.2/install/\n",
    "\n",
    "Download the weights trained on ImageNet for VGG:\n",
    "```\n",
    "wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n",
    "tar -xvf vgg_16_2016_08_28.tar.gz\n",
    "rm vgg_16_2016_08_28.tar.gz\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--train_dir', default='coco-animals/train')\n",
    "parser.add_argument('--train_dir', default='data/train-jpg/')\n",
    "# parser.add_argument('--val_dir', default='coco-animals/val')\n",
    "# parser.add_argument('--val_dir', default='data/')\n",
    "parser.add_argument('--model_path', default='vgg_16.ckpt', type=str)\n",
    "parser.add_argument('--batch_size', default=40, type=int) #32\n",
    "parser.add_argument('--num_workers', default=20, type=int) #4\n",
    "parser.add_argument('--num_epochs1', default=4, type=int) #10\n",
    "parser.add_argument('--num_epochs2', default=4, type=int) #10\n",
    "parser.add_argument('--learning_rate1', default=1e-3, type=float)\n",
    "parser.add_argument('--learning_rate2', default=1e-5, type=float)\n",
    "parser.add_argument('--dropout_keep_prob', default=0.5, type=float)\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float)\n",
    "\n",
    "\n",
    "VGG_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40479 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d8e3a8e6ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# for each filename and targets, read in pixel values from file, save image (resized to 32 x 32) to x, labels vector to y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/train-jpg/{}.jpg'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read in the data files \n",
    "x = []\n",
    "x_test = []\n",
    "y = [] # one hot encoding of which of the 17 tags this image is (images can have multiple tags)\n",
    "\n",
    "df_train = pd.read_csv('data/train_v2.csv') # column1: image_name, column2: tags (labels for image file)\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "# for each filename and targets, read in pixel values from file, save image (resized to 32 x 32) to x, labels vector to y\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x.append(cv2.resize(img, (32, 32)))\n",
    "    y.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array(y, np.uint8)\n",
    "x = np.array(x, np.float16) / 255.\n",
    "\n",
    "split = 35000\n",
    "x_train, x_valid, y_train, y_valid = x[:split], x[split:], y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view some images\n",
    "\n",
    "plt.imshow(x[0])\n",
    "plt.show()\n",
    "#plt.imshow(x[1])\n",
    "#plt.show()\n",
    "print(x[1].shape)\n",
    "plt.imshow(x[10][:, :, 0])\n",
    "plt.show()\n",
    "plt.imshow(x[10][:, :, 1])\n",
    "plt.show()\n",
    "plt.imshow(x[10][:, :, 2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('raw_image')\n",
    "img_ex = cv2.imread('data/train-jpg/{}.jpg'.format('train_0'))\n",
    "plt.imshow(img_ex)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(x[1])\n",
    "plt.show()\n",
    "\n",
    "img_ex = cv2.imread('data/train-jpg/{}.jpg'.format('train_1'))\n",
    "plt.imshow(img_ex)\n",
    "plt.show()\n",
    "\n",
    "print(y[0], y[1], y[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filenames_targets = pd.read_csv('data/train_v2.csv') # column1: image_name, column2: tags (labels for image file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\"\"\"\n",
    "# read in the data files \n",
    "x = []\n",
    "x_test = []\n",
    "y = [] # one hot encoding of which of the 17 tags this image is (images can have multiple tags)\n",
    "\n",
    "df_train = pd.read_csv('data/train_v2.csv') # column1: image_name, column2: tags (labels for image file)\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "# for each filename and targets, read in pixel values from file, save image (resized to 32 x 32) to x, labels vector to y\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x.append(cv2.resize(img, (32, 32)))\n",
    "    y.append(targets)\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "    Get all the images and labels from data/train_v2.csv\n",
    "    \"\"\"\n",
    "    filenames_targets = pd.read_csv('data/train_v2.csv') # column1: image_name, column2: tags (labels for image file)\n",
    "    filenames = [directory + file + '.jpg' for file in filenames_targets['image_name'].tolist()]\n",
    "    \n",
    "    labels = filenames_targets['tags'].tolist()\n",
    "\n",
    "    # for now just use integers instead of one hot encoding (with multiple ones possible)\n",
    "    # unique_labels = list(set(labels))\n",
    "\n",
    "    # label_to_int = {}\n",
    "    # for i, label in enumerate(unique_labels):\n",
    "    #     label_to_int[label] = i\n",
    "\n",
    "    # labels = [label_to_int[l] for l in labels]\n",
    "    \n",
    "    # Convert to one-hot labels\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    labels = list(set(flatten([l.split(' ') for l in filenames_targets['tags'].values])))\n",
    "    label_map = {l: i for i, l in enumerate(labels)}\n",
    "    inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "    one_hot_labels = []\n",
    "    \n",
    "    for f, tags in tqdm(filenames_targets.values, miniters=1000):\n",
    "        targets = [0]*17 #np.zeros(17)\n",
    "        for t in tags.split(' '):\n",
    "            targets[label_map[t]] = 1 \n",
    "        one_hot_labels.append(targets)\n",
    "    \n",
    "    return filenames, one_hot_labels\n",
    "    \n",
    "    '''\n",
    "    labels = os.listdir(directory)\n",
    "    files_and_labels = []\n",
    "    for label in labels:\n",
    "        for f in os.listdir(os.path.join(directory, label)):\n",
    "            files_and_labels.append((os.path.join(directory, label, f), label))\n",
    "\n",
    "    filenames, labels = zip(*files_and_labels)\n",
    "    filenames = list(filenames)\n",
    "    labels = list(labels)\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    label_to_int = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_int[label] = i\n",
    "\n",
    "    labels = [label_to_int[l] for l in labels]\n",
    "    '''\n",
    "\n",
    "# change to F score\n",
    "def check_accuracy(sess, correct_prediction, is_training, dataset_init_op):\n",
    "    \"\"\"\n",
    "    Check the accuracy of the model on either train or val (depending on dataset_init_op).\n",
    "    \"\"\"\n",
    "    # Initialize the correct dataset\n",
    "    sess.run(dataset_init_op)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    while True:\n",
    "        try:\n",
    "            correct_pred = sess.run(correct_prediction, {is_training: False})\n",
    "            num_correct += correct_pred.sum()\n",
    "            num_samples += correct_pred.shape[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # Return the fraction of datapoints that were correctly classified\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc\n",
    "\n",
    "\n",
    "def split_samples(all_filenames, all_labels):\n",
    "    \"\"\"\n",
    "    Split all filenames and labels into training and test sets, return both\n",
    "    \"\"\"\n",
    "    n = len(all_filenames)\n",
    "    order = random.sample(range(n), n)\n",
    "    all_filenames_random = [all_filenames[i] for i in order]\n",
    "    all_labels_random = [all_labels[i] for i in order]\n",
    "    \n",
    "    third = int(n/3)\n",
    "    val_filenames = all_filenames_random[:third]\n",
    "    val_labels = all_labels_random[:third]\n",
    "    train_filenames = all_filenames_random[third:]\n",
    "    train_labels = all_labels_random[third:]\n",
    "    \n",
    "    return train_filenames, train_labels, val_filenames, val_labels \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [00:00<00:00, 421483.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created\n",
      "got next batch\n",
      "INFO:tensorflow:Restoring parameters from vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Restoring parameters from vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 4\n",
      "Current loss: 13.020557\n",
      "Current loss: 9.494416\n",
      "Current loss: 9.266887\n",
      "Current loss: 7.931184\n",
      "Current loss: 8.027351\n",
      "Current loss: 8.142326\n",
      "Current loss: 9.702850\n",
      "Current loss: 8.159016\n",
      "Current loss: 9.700662\n",
      "Current loss: 9.093849\n",
      "Current loss: 10.039870\n",
      "Current loss: 10.191294\n",
      "Current loss: 10.603869\n",
      "Current loss: 10.660870\n",
      "Current loss: 12.616015\n",
      "Current loss: 13.469410\n",
      "Current loss: 11.645139\n",
      "Current loss: 9.253268\n",
      "Current loss: 15.476110\n",
      "Current loss: 12.592234\n",
      "Current loss: 10.178679\n",
      "Current loss: 16.079582\n",
      "Current loss: 13.703094\n",
      "Current loss: 13.503307\n",
      "Current loss: 15.821052\n",
      "Current loss: 14.554851\n",
      "Current loss: 14.258167\n",
      "Current loss: 16.114143\n",
      "Current loss: 17.408394\n",
      "Current loss: 18.609873\n",
      "Current loss: 14.635252\n",
      "Current loss: 20.348776\n",
      "Current loss: 18.531437\n",
      "Current loss: 17.410519\n",
      "Current loss: 21.604156\n",
      "Current loss: 17.998486\n",
      "Current loss: 22.909023\n",
      "Current loss: 19.567867\n",
      "Current loss: 17.626442\n",
      "Current loss: 18.477270\n",
      "Current loss: 18.823019\n",
      "Current loss: 16.614815\n",
      "Current loss: 22.022930\n",
      "Current loss: 21.276604\n",
      "Current loss: 17.496922\n",
      "Current loss: 20.751850\n",
      "Current loss: 22.397097\n",
      "Current loss: 23.616968\n",
      "Current loss: 20.067022\n",
      "Current loss: 20.093472\n",
      "Current loss: 22.088160\n",
      "Current loss: 25.989363\n",
      "Current loss: 23.058214\n",
      "Current loss: 18.866131\n",
      "Current loss: 25.866165\n",
      "Current loss: 26.151453\n",
      "Current loss: 23.901031\n",
      "Current loss: 25.661901\n",
      "Current loss: 28.836182\n",
      "Current loss: 26.623177\n",
      "Current loss: 21.327353\n",
      "Current loss: 21.592476\n",
      "Current loss: 22.137066\n",
      "Current loss: 28.339157\n",
      "Current loss: 27.478664\n",
      "Current loss: 33.442352\n",
      "Current loss: 30.991468\n",
      "Current loss: 24.695566\n",
      "Current loss: 27.544247\n",
      "Current loss: 25.382696\n",
      "Current loss: 25.102674\n",
      "Current loss: 37.966339\n",
      "Current loss: 32.903034\n",
      "Current loss: 22.224636\n",
      "Current loss: 24.364882\n",
      "Current loss: 27.350771\n",
      "Current loss: 32.589413\n",
      "Current loss: 29.683010\n",
      "Current loss: 27.236588\n",
      "Current loss: 33.985149\n",
      "Current loss: 34.656727\n",
      "Current loss: 27.606831\n",
      "Current loss: 23.950365\n",
      "Current loss: 33.872509\n",
      "Current loss: 36.406605\n",
      "Current loss: 38.141399\n",
      "Current loss: 39.644539\n",
      "Current loss: 38.032516\n",
      "Current loss: 32.442577\n",
      "Current loss: 47.488930\n",
      "Current loss: 26.839054\n",
      "Current loss: 36.213398\n",
      "Current loss: 35.887825\n",
      "Current loss: 33.722595\n",
      "Current loss: 28.016619\n",
      "Current loss: 32.772835\n",
      "Current loss: 43.478096\n",
      "Current loss: 25.563204"
     ]
    }
   ],
   "source": [
    "# Get the list of filenames and corresponding list of labels for training et validation\n",
    "# train_filenames, train_labels = list_images(args.train_dir)\n",
    "# val_filenames, val_labels = list_images(args.val_dir)\n",
    "\n",
    "\n",
    "all_filenames, all_labels = list_images(args.train_dir)\n",
    "\n",
    "train_filenames, train_labels, val_filenames, val_labels = split_samples(all_filenames, all_labels)\n",
    "\n",
    "num_classes = 17 #len(set(all_labels))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# In TensorFlow, you first want to define the computation graph with all the\n",
    "# necessary operations: loss, training op, accuracy...\n",
    "# Any tensor created in the `graph.as_default()` scope will be part of `graph`\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Standard preprocessing for VGG on ImageNet taken from here:\n",
    "    # https://github.com/tensorflow/models/blob/master/slim/preprocessing/vgg_preprocessing.py\n",
    "    # Also see the VGG paper for more details: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "    # Preprocessing (for both training and validation):\n",
    "    # (1) Decode the image from jpg format\n",
    "    # (2) Resize the image so its smaller side is 256 pixels long\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)          # (1)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "        smallest_side = 256.0\n",
    "        height, width = tf.shape(image)[0], tf.shape(image)[1]\n",
    "        height = tf.to_float(height)\n",
    "        width = tf.to_float(width)\n",
    "\n",
    "        scale = tf.cond(tf.greater(height, width),\n",
    "                        lambda: smallest_side / width,\n",
    "                         lambda: smallest_side / height)\n",
    "        new_height = tf.to_int32(height * scale)\n",
    "        new_width = tf.to_int32(width * scale)\n",
    "\n",
    "        resized_image = tf.image.resize_images(image, [new_height, new_width])  # (2)\n",
    "        return resized_image, label\n",
    "\n",
    "    # Preprocessing (for training)\n",
    "    # (3) Take a random 224x224 crop to the scaled image\n",
    "    # (4) Horizontally flip the image with probability 1/2\n",
    "    # (5) Substract the per color mean `VGG_MEAN`\n",
    "    # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "    def training_preprocess(image, label):\n",
    "        crop_image = tf.random_crop(image, [224, 224, 3])                       # (3)\n",
    "        flip_image = tf.image.random_flip_left_right(crop_image)                # (4)\n",
    "\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        centered_image = flip_image - means                                     # (5)\n",
    "\n",
    "        return centered_image, label\n",
    "\n",
    "    # Preprocessing (for validation)\n",
    "    # (3) Take a central 224x224 crop to the scaled image\n",
    "    # (4) Substract the per color mean `VGG_MEAN`\n",
    "    # Note: we don't normalize the data here, as VGG was trained without normalization\n",
    "    def val_preprocess(image, label):\n",
    "        crop_image = tf.image.resize_image_with_crop_or_pad(image, 224, 224)    # (3)\n",
    "\n",
    "        means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])\n",
    "        centered_image = crop_image - means                                     # (4)\n",
    "\n",
    "        return centered_image, label\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # DATASET CREATION using tf.contrib.data.Dataset\n",
    "    # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "\n",
    "    # The tf.contrib.data.Dataset framework uses queues in the background to feed in\n",
    "    # data to the model.\n",
    "    # We initialize the dataset with a list of filenames and labels, and then apply\n",
    "    # the preprocessing functions described above.\n",
    "    # Behind the scenes, queues will load the filenames, preprocess them with multiple\n",
    "    # threads and apply the preprocessing in parallel, and then batch the data\n",
    "\n",
    "    # Training dataset\n",
    "    train_filenames = tf.constant(train_filenames)\n",
    "    train_labels = tf.constant(train_labels)\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "    train_dataset = train_dataset.map(_parse_function,\n",
    "       num_threads=args.num_workers, output_buffer_size=args.batch_size)\n",
    "    train_dataset = train_dataset.map(training_preprocess,\n",
    "       num_threads=args.num_workers, output_buffer_size=args.batch_size)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)  # don't forget to shuffle\n",
    "    batched_train_dataset = train_dataset.batch(args.batch_size)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_filenames = tf.constant(val_filenames)\n",
    "    val_labels = tf.constant(val_labels)\n",
    "    val_dataset = tf.contrib.data.Dataset.from_tensor_slices((val_filenames, val_labels))\n",
    "    val_dataset = val_dataset.map(_parse_function,\n",
    "    num_threads=args.num_workers, output_buffer_size=args.batch_size)\n",
    "    val_dataset = val_dataset.map(val_preprocess,\n",
    "    num_threads=args.num_workers, output_buffer_size=args.batch_size)\n",
    "    batched_val_dataset = val_dataset.batch(args.batch_size)\n",
    "\n",
    "    print(\"dataset created\")\n",
    "    # Now we define an iterator that can operator on either dataset.\n",
    "    # The iterator can be reinitialized by calling:\n",
    "    #     - sess.run(train_init_op) for 1 epoch on the training set\n",
    "    #     - sess.run(val_init_op)   for 1 epoch on the valiation set\n",
    "    # Once this is done, we don't need to feed any value for images and labels\n",
    "    # as they are automatically pulled out from the iterator queues.\n",
    "\n",
    "    # A reinitializable iterator is defined by its structure. We could use the\n",
    "    # `output_types` and `output_shapes` properties of either `train_dataset`\n",
    "    # or `validation_dataset` here, because they are compatible.\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(batched_train_dataset.output_types,\n",
    "                                                       batched_train_dataset.output_shapes)\n",
    "    images, labels = iterator.get_next()\n",
    "    \n",
    "    print('got next batch')\n",
    "    train_init_op = iterator.make_initializer(batched_train_dataset)\n",
    "    val_init_op = iterator.make_initializer(batched_val_dataset)\n",
    "\n",
    "    # Indicates whether we are in training or in test mode\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Now that we have set up the data, it's time to set up the model.\n",
    "    # For this example, we'll use VGG-16 pretrained on ImageNet. We will remove the\n",
    "    # last fully connected layer (fc8) and replace it with our own, with an\n",
    "    # output size num_classes=8\n",
    "    # We will first train the last layer for a few epochs.\n",
    "    # Then we will train the entire model on our dataset for a few epochs.\n",
    "\n",
    "    # Get the pretrained model, specifying the num_classes argument to create a new\n",
    "    # fully connected replacing the last one, called \"vgg_16/fc8\"\n",
    "    # Each model has a different architecture, so \"vgg_16/fc8\" will change in another model.\n",
    "    # Here, logits gives us directly the predicted scores we wanted from the images.\n",
    "    # We pass a scope to initialize \"vgg_16/fc8\" weights with he_initializer\n",
    "    vgg = tf.contrib.slim.nets.vgg\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=args.weight_decay)):\n",
    "        logits, _ = vgg.vgg_16(images, num_classes=num_classes, is_training=is_training,\n",
    "                                   dropout_keep_prob=args.dropout_keep_prob)\n",
    "\n",
    "    # Specify where the model checkpoint is (pretrained weights).\n",
    "    model_path = args.model_path\n",
    "    assert(os.path.isfile(model_path))\n",
    "\n",
    "    # Restore only the layers up to fc7 (included)\n",
    "    # Calling function `init_fn(sess)` will load all the pretrained weights.\n",
    "    variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['vgg_16/fc8'])\n",
    "    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "\n",
    "    # Initialization operation from scratch for the new \"fc8\" layers\n",
    "    # `get_variables` will only return the variables whose name starts with the given pattern\n",
    "    fc8_variables = tf.contrib.framework.get_variables('vgg_16/fc8')\n",
    "    fc8_init = tf.variables_initializer(fc8_variables)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
    "    # We can then call the total loss easily\n",
    "    # tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits) \n",
    "    tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits) # softmax cross entropy loss so can have labels with multiple classes\n",
    "    loss = tf.losses.get_total_loss()  \n",
    "\n",
    "    # First we want to train only the reinitialized last layer fc8 for a few epochs.\n",
    "    # We run minimize the loss only with respect to the fc8 variables (weight and bias).\n",
    "    fc8_optimizer = tf.train.GradientDescentOptimizer(args.learning_rate1)\n",
    "    fc8_train_op = fc8_optimizer.minimize(loss, var_list=fc8_variables)\n",
    "\n",
    "    # Then we want to finetune the entire model for a few epochs.\n",
    "    # We run minimize the loss only with respect to all the variables.\n",
    "    full_optimizer = tf.train.GradientDescentOptimizer(args.learning_rate2)\n",
    "    full_train_op = full_optimizer.minimize(loss)\n",
    "\n",
    "    # Evaluation metrics\n",
    "       \n",
    "    # prediction = tf.to_int32(tf.argmax(logits, 1))\n",
    "    # correct_prediction = tf.equal(prediction, labels)\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    ########################\n",
    "    # best_F1_train = -1\n",
    "    # for t in np.arange(.1, .3, .01):\n",
    "    #    F1 = fbeta_score(y_train, np.array(p_train) > t, beta=2, average='samples')\n",
    "    #    if F1 > best_F1_train:\n",
    "    #        thresh = t\n",
    "    #        best_F1_train = F1\n",
    "\n",
    "    #p_valid = model.predict(x_valid, batch_size=128)\n",
    "    # thresh = 0.5\n",
    "    # F1 = fbeta_score(labels, np.array(logits) > thresh, beta=2, average='samples')\n",
    "    # print(\"F1\")\n",
    "    # print(F1)\n",
    "    \n",
    "    # print(\"Current results\", \"F1: \" , F1, \"threshold: \", thresh , \" Dropout: \", dProb, \" Filter Size: \", fSize)\n",
    "    # if F1 > best_F1:\n",
    "    #    best_f = fSize\n",
    "    #    best_d = dProb\n",
    "    #    best_thresh = thresh\n",
    "    #    best_F1 = F1\n",
    "    #    print(\"New best F1 found with\", \"F1: \" , best_F1, \"threshold: \", best_thresh , \" Dropout: \", best_d, \" Filter Size: \", best_f)\n",
    "\n",
    "    \n",
    "    tf.get_default_graph().finalize()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Now that we have built the graph and finalized it, we define the session.\n",
    "# The session is the interface to *run* the computational graph.\n",
    "# We can call our training operations with `sess.run(train_op)` for instance\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_fn(sess)  # load the pretrained weights\n",
    "    sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "\n",
    "    # Update only the last layer for a few epochs.\n",
    "    for epoch in range(args.num_epochs1):\n",
    "        # Run an epoch over the training data.\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, args.num_epochs1))\n",
    "        # Here we initialize the iterator with the training set.\n",
    "        # This means that we can go through an entire epoch until the iterator becomes empty.\n",
    "        sess.run(train_init_op)\n",
    "        while True:\n",
    "            try:\n",
    "                _, curr_loss, curr_logits, curr_labels = sess.run([fc8_train_op, loss, logits, labels], {is_training: True})\n",
    "                print('Current loss: %f' % curr_loss)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "        # Check F1 score on logits\n",
    "        thresh = 0.5\n",
    "        F1 = fbeta_score(curr_labels, np.array(curr_logits) > thresh, beta=2, average='samples')\n",
    "        print('F1: %f' % F1)\n",
    "        \n",
    "        # Check accuracy on the train and val sets every epoch.\n",
    "        # train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "        # val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "        # print('Train accuracy: %f' % train_acc)\n",
    "        # print('Val accuracy: %f\\n' % val_acc)\n",
    "\n",
    "\n",
    "    # Train the entire model for a few more epochs, continuing with the *same* weights.\n",
    "    for epoch in range(args.num_epochs2):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, args.num_epochs1))\n",
    "        sess.run(train_init_op)\n",
    "        while True:\n",
    "            try:\n",
    "                _, curr_loss, curr_logits, curr_labels = sess.run([full_train_op, loss, logits, labels], {is_training: True})\n",
    "                print('Current loss: %f' % curr_loss)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        # Check F1 score on logits\n",
    "        thresh = 0.5\n",
    "        F1 = fbeta_score(curr_labels, np.array(curr_logits) > thresh, beta=2, average='samples')\n",
    "        print('F1: %f' % F1)\n",
    "        \n",
    "        # Check accuracy on the train and val sets every epoch\n",
    "        # train_acc = check_accuracy(sess, correct_prediction, is_training, train_init_op)\n",
    "        # val_acc = check_accuracy(sess, correct_prediction, is_training, val_init_op)\n",
    "        # print('Train accuracy: %f' % train_acc)\n",
    "        # print('Val accuracy: %f\\n' % val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
