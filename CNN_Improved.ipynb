{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline small CNN for project Milestone\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keras libraries\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras import backend\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [03:43<00:00, 180.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# referred to https://www.kaggle.com/anokas/simple-keras-starter for help reading data and setting up basic Keras model\n",
    "x = []\n",
    "x_test = []\n",
    "y = []\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('/home/joerj/train-jpg/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x.append(cv2.resize(img, (40, 40)))\n",
    "    y.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Flip images - but need to keep seperate from validation and training \n",
    "#for i in range(len(x)):\n",
    "#    x.append(np.fliplr(x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y, np.uint8)\n",
    "x_train = np.array(x, np.float16) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create model class - model outline sourced from here: https://github.com/EKami/planet-amazon-deforestation\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "class AmazonClassifier:\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.classifier = Sequential()\n",
    "    def add_conv_layer_init(self, img_size=(32, 32), c=3, f = 32, p = .25):\n",
    "        self.classifier.add(BatchNormalization(input_shape=(*img_size, c)))\n",
    "        self.classifier.add(Conv2D(f, kernel_size=(3, 3),\n",
    "                         padding = 'same',\n",
    "                         activation='relu'))\n",
    "        self.classifier.add(Conv2D(f, (3, 3), activation='relu'))        \n",
    "        self.classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.classifier.add(Dropout(p))\n",
    "        \n",
    "    def add_conv_layer_mid(self, img_size=(32, 32), c=3, f = 32, p = .25):\n",
    "        self.classifier.add(Conv2D(f, kernel_size=(3, 3),\n",
    "                         padding = 'same',\n",
    "                         activation='relu'))\n",
    "        self.classifier.add(Conv2D(f, (3, 3), activation='relu'))        \n",
    "        self.classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.classifier.add(Dropout(p))\n",
    "\n",
    "    def _get_fbeta_score(self, classifier, X_valid, y_valid):\n",
    "        p_valid = classifier.predict(X_valid)\n",
    "        return fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples')\n",
    "\n",
    "    def add_flatten_layer(self):\n",
    "        self.classifier.add(Flatten())\n",
    "\n",
    "    def add_dense_layer(self, output_size = 17, p = 0.5):\n",
    "        self.classifier.add(Dense(512, activation='relu'))\n",
    "        self.classifier.add(BatchNormalization())\n",
    "        self.classifier.add(Dropout(0.5))\n",
    "        self.classifier.add(Dense(output_size, activation='sigmoid'))\n",
    "        \n",
    "    def train_model(self, x_train, y_train, learn_rate=0.001, epoch=5, batch_size=128, validation_split_size=0.2, train_callbacks=()):\n",
    "        history = LossHistory()\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train,\n",
    "                                                              test_size=validation_split_size)\n",
    "\n",
    "        opt = Adam(lr=learn_rate)\n",
    "\n",
    "        self.classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # early stopping will auto-stop training process if model stops learning after 3 epochs\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "\n",
    "        self.classifier.fit(X_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epoch,\n",
    "                            verbose=1,\n",
    "                            validation_data=(X_valid, y_valid),\n",
    "                            callbacks=[history, *train_callbacks, earlyStopping])\n",
    "        fbeta_score = self._get_fbeta_score(self.classifier, X_valid, y_valid)\n",
    "        return [history.train_losses, history.val_losses, fbeta_score]\n",
    "    \n",
    "    def save_weights(self, weight_file_path):\n",
    "        self.classifier.save_weights(weight_file_path)\n",
    "\n",
    "    def load_weights(self, weight_file_path):\n",
    "        self.classifier.load_weights(weight_file_path)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        predictions = self.classifier.predict(x_test)\n",
    "        return predictions\n",
    "\n",
    "    def map_predictions(self, predictions, labels_map, thresholds):\n",
    "        \"\"\"\n",
    "        Return the predictions mapped to their labels\n",
    "        :param predictions: the predictions from the predict() method\n",
    "        :param labels_map: the map\n",
    "        :param thresholds: The threshold of each class to be considered as existing or not existing\n",
    "        :return: the predictions list mapped to their labels\n",
    "        \"\"\"\n",
    "        predictions_labels = []\n",
    "        for prediction in predictions:\n",
    "            labels = [labels_map[i] for i, value in enumerate(prediction) if value > thresholds[i]]\n",
    "            predictions_labels.append(labels)\n",
    "\n",
    "        return predictions_labels\n",
    "\n",
    "    def close(self):\n",
    "        backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9190Epoch 00000: val_acc improved from -inf to 0.93724, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 61s - loss: 0.2054 - acc: 0.9190 - val_loss: 0.1591 - val_acc: 0.9372\n",
      "Epoch 2/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9330Epoch 00001: val_acc improved from 0.93724 to 0.93892, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 53s - loss: 0.1686 - acc: 0.9330 - val_loss: 0.1535 - val_acc: 0.9389\n",
      "Epoch 3/5\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9364Epoch 00002: val_acc improved from 0.93892 to 0.93946, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1605 - acc: 0.9364 - val_loss: 0.1478 - val_acc: 0.9395\n",
      "Epoch 4/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9397Epoch 00003: val_acc improved from 0.93946 to 0.94219, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1522 - acc: 0.9397 - val_loss: 0.1463 - val_acc: 0.9422\n",
      "Epoch 5/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9414Epoch 00004: val_acc improved from 0.94219 to 0.94744, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 53s - loss: 0.1494 - acc: 0.9414 - val_loss: 0.1333 - val_acc: 0.9474\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9425Epoch 00000: val_acc improved from 0.94744 to 0.94909, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 51s - loss: 0.1472 - acc: 0.9425 - val_loss: 0.1313 - val_acc: 0.9491\n",
      "Epoch 2/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9438Epoch 00001: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1436 - acc: 0.9438 - val_loss: 0.1341 - val_acc: 0.9455\n",
      "Epoch 3/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9454Epoch 00002: val_acc improved from 0.94909 to 0.95059, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1409 - acc: 0.9454 - val_loss: 0.1229 - val_acc: 0.9506\n",
      "Epoch 4/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9462Epoch 00003: val_acc improved from 0.95059 to 0.95225, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1381 - acc: 0.9462 - val_loss: 0.1210 - val_acc: 0.9522\n",
      "Epoch 5/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9462Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 50s - loss: 0.1379 - acc: 0.9462 - val_loss: 0.1229 - val_acc: 0.9518\n",
      "Epoch 6/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9476Epoch 00005: val_acc did not improve\n",
      "35479/35479 [==============================] - 49s - loss: 0.1349 - acc: 0.9476 - val_loss: 0.1418 - val_acc: 0.9448\n",
      "Epoch 7/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9486Epoch 00006: val_acc did not improve\n",
      "35479/35479 [==============================] - 50s - loss: 0.1323 - acc: 0.9486 - val_loss: 0.1272 - val_acc: 0.9502\n",
      "Epoch 8/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9486Epoch 00007: val_acc improved from 0.95225 to 0.95488, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 50s - loss: 0.1329 - acc: 0.9486 - val_loss: 0.1141 - val_acc: 0.9549\n",
      "Epoch 9/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9489Epoch 00008: val_acc did not improve\n",
      "35479/35479 [==============================] - 52s - loss: 0.1313 - acc: 0.9489 - val_loss: 0.1198 - val_acc: 0.9536\n",
      "Epoch 10/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9448Epoch 00009: val_acc improved from 0.95488 to 0.95500, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 51s - loss: 0.1422 - acc: 0.9448 - val_loss: 0.1163 - val_acc: 0.9550\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9521Epoch 00000: val_acc improved from 0.95500 to 0.95828, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1241 - acc: 0.9521 - val_loss: 0.1061 - val_acc: 0.9583\n",
      "Epoch 2/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9531Epoch 00001: val_acc improved from 0.95828 to 0.95881, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 50s - loss: 0.1216 - acc: 0.9531 - val_loss: 0.1049 - val_acc: 0.9588\n",
      "Epoch 3/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9533Epoch 00002: val_acc improved from 0.95881 to 0.95886, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 51s - loss: 0.1206 - acc: 0.9533 - val_loss: 0.1045 - val_acc: 0.9589\n",
      "Epoch 4/5\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9532Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 52s - loss: 0.1211 - acc: 0.9532 - val_loss: 0.1043 - val_acc: 0.9589\n",
      "Epoch 5/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9540Epoch 00004: val_acc improved from 0.95886 to 0.95905, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1187 - acc: 0.9540 - val_loss: 0.1044 - val_acc: 0.9590\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9539Epoch 00000: val_acc improved from 0.95905 to 0.95962, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 53s - loss: 0.1191 - acc: 0.9539 - val_loss: 0.1042 - val_acc: 0.9596\n",
      "Epoch 2/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9543Epoch 00001: val_acc did not improve\n",
      "35479/35479 [==============================] - 52s - loss: 0.1185 - acc: 0.9543 - val_loss: 0.1045 - val_acc: 0.9593\n",
      "Epoch 3/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9544Epoch 00002: val_acc improved from 0.95962 to 0.96016, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 51s - loss: 0.1177 - acc: 0.9544 - val_loss: 0.1038 - val_acc: 0.9602\n",
      "Epoch 4/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9548Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1176 - acc: 0.9548 - val_loss: 0.1031 - val_acc: 0.9601\n",
      "Epoch 5/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9548Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 52s - loss: 0.1168 - acc: 0.9548 - val_loss: 0.1042 - val_acc: 0.9594\n",
      "Epoch 6/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9553Epoch 00005: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1163 - acc: 0.9553 - val_loss: 0.1049 - val_acc: 0.9597\n",
      "Epoch 7/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9548Epoch 00006: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1160 - acc: 0.9548 - val_loss: 0.1047 - val_acc: 0.9600\n",
      "Epoch 8/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9557Epoch 00007: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1155 - acc: 0.9557 - val_loss: 0.1043 - val_acc: 0.9600\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9556Epoch 00000: val_acc improved from 0.96016 to 0.96129, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1150 - acc: 0.9556 - val_loss: 0.0969 - val_acc: 0.9613\n",
      "Epoch 2/5\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9556Epoch 00001: val_acc improved from 0.96129 to 0.96147, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1150 - acc: 0.9556 - val_loss: 0.0965 - val_acc: 0.9615\n",
      "Epoch 3/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9557Epoch 00002: val_acc improved from 0.96147 to 0.96153, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 53s - loss: 0.1151 - acc: 0.9556 - val_loss: 0.0963 - val_acc: 0.9615\n",
      "Epoch 4/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9556Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 53s - loss: 0.1148 - acc: 0.9556 - val_loss: 0.0969 - val_acc: 0.9615\n",
      "Epoch 5/5\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9558Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 53s - loss: 0.1144 - acc: 0.9558 - val_loss: 0.0967 - val_acc: 0.9613\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9561Epoch 00000: val_acc improved from 0.96153 to 0.96162, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 53s - loss: 0.1136 - acc: 0.9561 - val_loss: 0.1013 - val_acc: 0.9616\n",
      "Epoch 2/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9560Epoch 00001: val_acc improved from 0.96162 to 0.96180, saving model to weights.best.hdf5\n",
      "35479/35479 [==============================] - 52s - loss: 0.1138 - acc: 0.9560 - val_loss: 0.1020 - val_acc: 0.9618\n",
      "Epoch 3/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9558Epoch 00002: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1145 - acc: 0.9558 - val_loss: 0.1023 - val_acc: 0.9616\n",
      "Epoch 4/10\n",
      "35440/35479 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9558Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1145 - acc: 0.9558 - val_loss: 0.1018 - val_acc: 0.9617\n",
      "Epoch 5/10\n",
      "35472/35479 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9556Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 51s - loss: 0.1141 - acc: 0.9556 - val_loss: 0.1014 - val_acc: 0.9615\n"
     ]
    }
   ],
   "source": [
    "#Grid search from ekami - works pretty well\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size = 16\n",
    "validation_split_size = 5000\n",
    "classifier = AmazonClassifier()\n",
    "classifier.add_conv_layer_init(img_size=(40, 40))\n",
    "classifier.add_conv_layer_mid(f = 64)\n",
    "classifier.add_conv_layer_mid(f = 128)\n",
    "classifier.add_flatten_layer()\n",
    "classifier.add_dense_layer()\n",
    "\n",
    "train_losses, val_losses, scores_list = [], [], []\n",
    "\n",
    "epochs_arr = [5, 10]\n",
    "learn_rates = [0.001, 0.0001, 0.00001]\n",
    "learn_rates = [0.00001]\n",
    "for learn_rate in learn_rates:\n",
    "    for epochs in epochs_arr:\n",
    "        tmp_train_losses, tmp_val_losses, score = classifier.train_model(x_train, y_train, learn_rate, epochs, \n",
    "                                                                               batch_size, validation_split_size=validation_split_size, \n",
    "                                                                               train_callbacks=[checkpoint])\n",
    "        train_losses += tmp_train_losses\n",
    "        val_losses += tmp_val_losses\n",
    "        scores_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.88234807134939619,\n",
       " 0.90041064609876198,\n",
       " 0.91140191131703252,\n",
       " 0.91251613410072496,\n",
       " 0.91906292405795087,\n",
       " 0.91561545143151457]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.88278592403758316,\n",
       " 0.89380226479423941,\n",
       " 0.91335860719395501,\n",
       " 0.91403794224670998,\n",
       " 0.91967140438118244,\n",
       " 0.91611982708544204]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict on the test set \n",
    "# Implemenet best threshold selection? \n",
    "x_test, x_test_filename = data_helper.preprocess_test_data(test_jpeg_dir, img_resize)\n",
    "x_test = []\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('/home/joerj/test-jpg/test-jpg/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21665956302257908 0.592042238360719 128 5 1.9527295540425954e-06\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.5096Epoch 00000: val_acc did not improve\n",
      "35479/35479 [==============================] - 13s - loss: 0.8400 - acc: 0.5096 - val_loss: 0.6894 - val_acc: 0.4610\n",
      "Epoch 2/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.7891 - acc: 0.5185Epoch 00001: val_acc did not improve\n",
      "35479/35479 [==============================] - 12s - loss: 0.7891 - acc: 0.5185 - val_loss: 0.6867 - val_acc: 0.5387\n",
      "Epoch 3/5\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.7504 - acc: 0.5279Epoch 00002: val_acc did not improve\n",
      "35479/35479 [==============================] - 12s - loss: 0.7503 - acc: 0.5280 - val_loss: 0.6811 - val_acc: 0.5826\n",
      "Epoch 4/5\n",
      "35328/35479 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.5387Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 12s - loss: 0.7233 - acc: 0.5387 - val_loss: 0.6722 - val_acc: 0.6039\n",
      "Epoch 5/5\n",
      "35328/35479 [============================>.] - ETA: 0s - loss: 0.7065 - acc: 0.5478Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 12s - loss: 0.7065 - acc: 0.5478 - val_loss: 0.6654 - val_acc: 0.6336\n",
      "0.29499934314735166 0.5882289849203507 64 10 9.520549465625149e-05\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8019Epoch 00000: val_acc did not improve\n",
      "35479/35479 [==============================] - 17s - loss: 0.5551 - acc: 0.8022 - val_loss: 0.3506 - val_acc: 0.9118\n",
      "Epoch 2/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9257Epoch 00001: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.2440 - acc: 0.9257 - val_loss: 0.1984 - val_acc: 0.9259\n",
      "Epoch 3/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9312Epoch 00002: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1837 - acc: 0.9312 - val_loss: 0.1775 - val_acc: 0.9310\n",
      "Epoch 4/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9338Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1703 - acc: 0.9338 - val_loss: 0.1667 - val_acc: 0.9342\n",
      "Epoch 5/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9362Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1622 - acc: 0.9362 - val_loss: 0.1529 - val_acc: 0.9397\n",
      "Epoch 6/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9386Epoch 00005: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1564 - acc: 0.9386 - val_loss: 0.1553 - val_acc: 0.9392\n",
      "Epoch 7/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9397Epoch 00006: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1528 - acc: 0.9397 - val_loss: 0.1510 - val_acc: 0.9412\n",
      "Epoch 8/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9411Epoch 00007: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1493 - acc: 0.9411 - val_loss: 0.1469 - val_acc: 0.9418\n",
      "Epoch 9/10\n",
      "35456/35479 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9424Epoch 00008: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1459 - acc: 0.9424 - val_loss: 0.1481 - val_acc: 0.9416\n",
      "Epoch 10/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9436Epoch 00009: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.1430 - acc: 0.9436 - val_loss: 0.1398 - val_acc: 0.9456\n",
      "0.20420354768817378 0.5333605950445897 64 10 6.586971472684133e-06\n",
      "Train on 35479 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.7463 - acc: 0.5314Epoch 00000: val_acc did not improve\n",
      "35479/35479 [==============================] - 17s - loss: 0.7461 - acc: 0.5315 - val_loss: 0.6740 - val_acc: 0.6317\n",
      "Epoch 2/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.6750 - acc: 0.5962Epoch 00001: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.6749 - acc: 0.5963 - val_loss: 0.6513 - val_acc: 0.7202\n",
      "Epoch 3/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.6624Epoch 00002: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.6515 - acc: 0.6624 - val_loss: 0.6291 - val_acc: 0.8490\n",
      "Epoch 4/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.7231Epoch 00003: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.6315 - acc: 0.7232 - val_loss: 0.6055 - val_acc: 0.8872\n",
      "Epoch 5/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.7768Epoch 00004: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.6103 - acc: 0.7769 - val_loss: 0.5826 - val_acc: 0.8989\n",
      "Epoch 6/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8202Epoch 00005: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.5862 - acc: 0.8202 - val_loss: 0.5556 - val_acc: 0.9056\n",
      "Epoch 7/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8508Epoch 00006: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.5580 - acc: 0.8508 - val_loss: 0.5250 - val_acc: 0.9119\n",
      "Epoch 8/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8724Epoch 00007: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.5278 - acc: 0.8724 - val_loss: 0.4958 - val_acc: 0.9166\n",
      "Epoch 9/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8881Epoch 00008: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.4950 - acc: 0.8881 - val_loss: 0.4629 - val_acc: 0.9196\n",
      "Epoch 10/10\n",
      "35392/35479 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8996Epoch 00009: val_acc did not improve\n",
      "35479/35479 [==============================] - 16s - loss: 0.4617 - acc: 0.8997 - val_loss: 0.4293 - val_acc: 0.9222\n"
     ]
    }
   ],
   "source": [
    "# Try to improve via Random hyperparameter search \n",
    "validation_split_size = 5000\n",
    "num_experiments = 3\n",
    "\n",
    "for i in range(num_experiments):         \n",
    "    p_conv = np.random.uniform(low = 0.2, high = 0.3)\n",
    "    p_all = np.random.uniform(low = 0.4, high = 0.6)\n",
    "    batch_size = np.random.choice((64, 128))\n",
    "    \n",
    "    classifier = AmazonClassifier()\n",
    "    classifier.add_conv_layer_init(f = 32, p = p_conv)\n",
    "    classifier.add_conv_layer_mid(f = 64, p = p_conv)\n",
    "    classifier.add_conv_layer_mid(f = 128, p = p_conv)\n",
    "    classifier.add_flatten_layer()\n",
    "    classifier.add_dense_layer(p = p_all)\n",
    "\n",
    "    train_losses, val_losses, scores_list = [], [], []\n",
    "\n",
    "    epochs = np.random.choice((5, 10))\n",
    "    learn_rate = 10**(np.random.uniform(low = -5, high = -3))\n",
    "\n",
    "    print(p_conv, p_all, batch_size, epochs, learn_rate)\n",
    "    tmp_train_losses, tmp_val_losses, score = classifier.train_model(x_train, y_train, learn_rate, epochs, \n",
    "                                                                           batch_size, validation_split_size=validation_split_size, \n",
    "                                                                           train_callbacks=[checkpoint])\n",
    "    train_losses += tmp_train_losses\n",
    "    val_losses += tmp_val_losses\n",
    "    scores_list.append(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
